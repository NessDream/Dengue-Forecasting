{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les tutos de xavier Dupré se trouvent [ici](http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx3/notebooks/gradient_boosting.html#gradientboostingrst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "Le notebook explore l'algorithme du [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jyquickhelper\n",
    "!pip install matplotlib\n",
    "!pip install sklearn\n",
    "!pip install tqdm\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jyquickhelper import add_notebook_menu\n",
    "add_notebook_menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premier exemple\n",
    "\n",
    "On considère les paramètres par défaut de la classe [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn, random\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "rnd = randn(1000)\n",
    "X = random(1000) * 8 - 4\n",
    "y = X ** 2 - X + rnd * 2 + 150 # X^2 - X + 150 + epsilon\n",
    "X = X.reshape((-1, 1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "df = DataFrame({'X': X_train.ravel(), 'y': y_train})\n",
    "ax = df.plot(x='X', y='y', kind='scatter')\n",
    "ax.set_title(\"Nuage de points X^2 - X + 150 + epsilon\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor(max_depth=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "ind = numpy.argsort(X_test, axis=0)\n",
    "y_ = model.predict(X_test)\n",
    "df = DataFrame({'X': X_test[ind].ravel(), \n",
    "                'y': y_test[ind].ravel(),\n",
    "                'y^': y_[ind].ravel()})\n",
    "ax = df.plot(x='X', y='y', kind='scatter')\n",
    "df.plot(x='X', y='y^', kind='line', ax=ax, color=\"r\")\n",
    "ax.set_title(\"Prédictions avec GradientBoostingRegressor\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rien d'imprévu jusque là. Essayons autre chose. On regarde avec une seule itération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(max_depth=1, n_estimators=1, learning_rate=0.5)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = model.predict(X_test)\n",
    "df = DataFrame({'X': X_test[ind].ravel(), \n",
    "                'y': y_test[ind].ravel(),\n",
    "                'y^': y_[ind].ravel()})\n",
    "ax = df.plot(x='X', y='y', kind='scatter')\n",
    "df.plot(x='X', y='y^', kind='line', ax=ax, color=\"r\")\n",
    "ax.set_title(\"Prédictions avec GradientBoostingRegressor\\net une fonction en escalier\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons de montrer l'évolution de la courbe prédite en fonction du nombre de marches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, model.estimators_.shape[0] + 1, 10):\n",
    "    if i == 0:\n",
    "        df = DataFrame({'X': X_test[ind].ravel(),\n",
    "                        'y': y_test[ind].ravel()})\n",
    "        ax = df.plot(x='X', y='y', kind='scatter', figsize=(10, 4))\n",
    "        y_ = model.init_.predict(X_test)\n",
    "        color = 'b'\n",
    "    else:\n",
    "        y_ = sum([model.init_.predict(X_test)] + \n",
    "                 [model.estimators_[k, 0].predict(X_test) * model.learning_rate\n",
    "                  for k in range(0, i)])\n",
    "        color = 'r'\n",
    "    df = DataFrame({'X': X_test[ind].ravel(),\n",
    "                    'y^': y_[ind].ravel()})\n",
    "    df.plot(x='X', y='y^', kind='line', ax=ax, color=color, label='i=%d' % i)\n",
    "ax.set_title(\"Prédictions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning rate et itérations\n",
    "\n",
    "Et si on choisissait un *learning_rate*, plus petit ou plus grand..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model01 = GradientBoostingRegressor(max_depth=1, learning_rate=0.01)\n",
    "model01.fit(X_train, y_train)\n",
    "modela = GradientBoostingRegressor(max_depth=1, learning_rate=1.2)\n",
    "modela.fit(X_train, y_train)\n",
    "modelb = GradientBoostingRegressor(max_depth=1, learning_rate=1.99)\n",
    "modelb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ind = numpy.argsort(X_test, axis=0)\n",
    "\n",
    "for i, mod in enumerate([model01, modela, modelb]):\n",
    "    df = DataFrame({'X': X_test[ind].ravel(), \n",
    "                    'y': y_test[ind].ravel(),\n",
    "                    'y^': mod.predict(X_test)[ind].ravel()})\n",
    "    df.plot(x='X', y='y', kind='scatter', ax=ax[i])\n",
    "    df.plot(x='X', y='y^', kind='line', ax=ax[i], color=\"r\")\n",
    "    ax[i].set_title(\"learning_rate=%f\" % mod.learning_rate);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une trop faible valeur de *learning_rate* semble retenir le modèle de converger, une grande valeur produit des effets imprévisibles. Pour comprendre pourquoi, il faut détailler l'algorithme..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'algorithme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspiration\n",
    "\n",
    "\n",
    "L'algorithme est inspiré de l'algorithme de la descente de gradient. On considère une fonction réelle $f(x)$ et on calcule le gradient $\\frac{\\partial f}{\\partial x}(x)$ pour construire une suite :\n",
    "\n",
    "$$x_{t+1} = x_t - \\epsilon_t \\frac{\\partial f}{\\partial x}(x_t)$$\n",
    "\n",
    "La suite $(x_t)$ converge vers le minimum de la fonction $f$. On applique cela à une fonction d'erreur issue d'un problème de régression.\n",
    "\n",
    "$$f(x) = \\sum_{n=1}^N l(F(X_i), y_i)$$\n",
    "\n",
    "Le plus souvent, on applique cette méthode à une fonction $F$ qui dépend d'un paramètre $\\theta$\n",
    "\n",
    "$$f(\\theta, x) = \\sum_{n=1}^N l(F(\\theta, X_i), y_i)$$\n",
    "\n",
    "Et c'est la suite $\\theta_{t+1} = \\theta_t - \\epsilon_t \\frac{\\partial f}{\\partial \\theta}(\\theta_t)$ qui converge vers le minimum de la fonction $f$ de sorte que la fonction $f(\\theta, x)$ approxime au mieux les points $(X_i, y_i)$. Mais, on pourrait tout-à-fait résoudre ce problème dans un espace de fonctions et non un espace de paramètres :\n",
    "\n",
    "$$G_{t+1} = G_t - \\epsilon_t \\frac{\\partial f}{\\partial G}(G_t)$$\n",
    "\n",
    "Le gradient $\\frac{\\partial f}{\\partial G}$ est facile à calculer puisqu'il ne dépend pas de $G$. On pourrait donc construire la fonction de régression $G$ comme une suite additive de fonctions $F_k \\sim - \\epsilon_t \\frac{\\partial f}{\\partial G}(G_t)$. \n",
    "\n",
    "$$G_t = \\sum_{k=1}^t F_k$$\n",
    "\n",
    "Et nous pourrions construire la fonction $F_k$ comme solution d'un problème de régression défini par les couples $(X_i, z_i)$ avec :\n",
    "\n",
    "$$\\begin{array}{rcl} z_i &=& - \\epsilon_t \\frac{\\partial f}{\\partial G}(G_t(X_i), y_i) \\\\ f(X_i, y_i) &=& l(G_t(X_i), y_i)\\end{array}$$\n",
    "\n",
    "Voilà l'idée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme\n",
    "\n",
    "Je reprends ici la page wikipedia. On cherche à construire un modèle qui minimise l'erreur $L(y,F(x)) = \\sum_{i=1}^n l(y_i, F(X_i))$. On note $r$ le learning rate.\n",
    "\n",
    "**Etape 1 :** on cale un premier modèle de régression, ici, simplement une constante, en optimisant $F_0(x) = \\arg \\min_\\gamma \\sum_{i=1}^n L(y_i, \\gamma)$. $F_0(x)$ est une constante.\n",
    "\n",
    "On note ensuite $F_m(x) = \\gamma_0 \\sum_{k=1}^m r \\gamma_k h_k(x)$ où $\\gamma_0$ est la fonction constante construire lors de la première étape.\n",
    "\n",
    "**Etape 2 :** on calcule ensuite les erreurs $e_{im} = l(y_i, F_m(x_i))$ et l'opposé du gradient $r_{im} = - \\left[ \\frac{\\partial l(y_i, F_m(x_i)) }{\\partial F_m(x_i)} \\right]$\n",
    "\n",
    "**Etape 3 :** on choisit la fonction $h_{m+1}(x)$ de telle sorte qu'elle approxime au mieux les résidus $r_{im}$.\n",
    "\n",
    "**Etape 4 :** on choisit le coefficient $\\gamma_{m+1}$ de telle sorte qu'il minimise l'expression $\\min_\\gamma \\sum_{i=1}^n l\\left(y_i, \\gamma_0 + \\sum_{k=1}^m r \\gamma_k h_k(x_i) + \\gamma h_{m+1}(x_i)\\right)$.\n",
    "\n",
    "On retourne l'étape 2 autant de fois qu'il y a d'itérations. Lorsque l'erreur est une erreur quadratique $l(y, F(x)) = (y-F(x))^2$, les résidus deviennent $r_{im} = -2 (y_i - F_m(x_i))$. Par conséquent, la fonction $h$ approxime au mieux ce qu'il manque pour atteindre l'objectif. Un learning rate égal à 1 fait que la somme des prédictions de chaque fonction $h_m$ oscille autour de la vraie valeur, une faible valeur donne l'impression d'une fonction qui converge à petits pas, une grande valeur accroît l'amplitude des oscillations au point d'empêcher l'algorithme de converger.\n",
    "\n",
    "On voit aussi que l'algorithme s'intéresse d'abord aux points où le gradient est le plus fort, donc en principe aux erreurs les plus grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression quantile\n",
    "\n",
    "Dans ce cas, l'erreur quadratique est remplacée par une erreur en valeur absolue. Les résidus dans ce cas sont égaux à -1 ou 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "model = GradientBoostingRegressor(alpha=alpha, loss='quantile', max_depth=1, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "model01 = GradientBoostingRegressor(alpha=alpha, loss='quantile', max_depth=1, learning_rate=0.01)\n",
    "model01.fit(X_train, y_train)\n",
    "modela = GradientBoostingRegressor(alpha=alpha, loss='quantile', max_depth=1, learning_rate=1.2)\n",
    "modela.fit(X_train, y_train)\n",
    "modelb = GradientBoostingRegressor(alpha=alpha, loss='quantile', max_depth=1, learning_rate=1.99)\n",
    "modelb.fit(X_train, y_train)\n",
    "modelc = GradientBoostingRegressor(alpha=alpha, loss='quantile', max_depth=1, learning_rate=2.01)\n",
    "modelc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5, figsize=(12, 4))\n",
    "ind = numpy.argsort(X_test, axis=0)\n",
    "\n",
    "for i, mod in enumerate([model, model01, modela, modelb, modelc]):\n",
    "    df = DataFrame({'X': X_test[ind].ravel(), \n",
    "                    'y': y_test[ind].ravel(),\n",
    "                    'y^': mod.predict(X_test)[ind].ravel()})\n",
    "    df.plot(x='X', y='y', kind='scatter', ax=ax[i])\n",
    "    df.plot(x='X', y='y^', kind='line', ax=ax[i], color=\"r\")\n",
    "    ax[i].set_title(\"learning_rate=%1.2f\" % mod.learning_rate);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concrètement, le paramètre *max_depth=1* correspond à une simple fonction $f(x) = \\mathbb{1}_{x > s}$ et le modèle final est une somme pondérée de fonctions indicatrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning_rate et sur-apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "def experiment(models, tries=25):\n",
    "    scores = []\n",
    "    for _ in tqdm(range(tries)):\n",
    "        rnd = randn(1000)\n",
    "        X = random(1000) * 8 - 4\n",
    "        y = X ** 2 - X + rnd * 2 + 150 # X^2 - X + 150 + epsilon\n",
    "        X = X.reshape((-1, 1))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "        scs = []\n",
    "        for model in models:\n",
    "            model.fit(X_train, y_train)\n",
    "            sc = model.score(X_test, y_test)\n",
    "            scs.append(sc)\n",
    "        scores.append(scs)\n",
    "    return scores\n",
    "\n",
    "scores = experiment([\n",
    "    GradientBoostingRegressor(max_depth=1, n_estimators=100),\n",
    "    RandomForestRegressor(max_depth=1, n_estimators=100)\n",
    "])\n",
    "scores[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot([_[0] for _ in scores], label=\"GradientBoostingRegressor\")\n",
    "ax.plot([_[1] for _ in scores], label=\"RandomForestRegressor\")\n",
    "ax.set_title(\"Comparaison pour une somme pondérée de fonctions en escalier\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce résultat est attendu car la forêt aléatoire est une moyenne de modèle de régression tous appris dans les mêmes conditions alors que le gradient boosting s'intéresse à l'erreur après la somme des premiers régresseurs. Voyons avec des arbres de décision et non plus des fonctions en escaliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "scores = experiment([\n",
    "    GradientBoostingRegressor(max_depth=5, n_estimators=100),\n",
    "    RandomForestRegressor(max_depth=5, n_estimators=100),\n",
    "    DecisionTreeRegressor(max_depth=5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot([_[0] for _ in scores], label=\"GradientBoostingRegressor\")\n",
    "ax.plot([_[1] for _ in scores], label=\"RandomForestRegressor\")\n",
    "ax.plot([_[2] for _ in scores], label=\"DecisionTreeRegressor\")\n",
    "ax.set_title(\"Comparaison pour une somme pondérée d'arbres de décisions\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle *GradientBoostingRegressor* est clairement moins bon quand le modèle sous-jacent - l'arbre de décision - est performant. On voit que la forêt aléatoire est meilleure qu'un arbre de décision seul. Cela signifie qu'elle généralise mieux et que l'arbre de décision fait du sur apprentissage. De même, le *GradientBoostingRegressor* est plus exposé au sur-apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = experiment([\n",
    "    RandomForestRegressor(max_depth=5, n_estimators=100),\n",
    "    GradientBoostingRegressor(max_depth=5, n_estimators=100, learning_rate=0.05),\n",
    "    GradientBoostingRegressor(max_depth=5, n_estimators=100, learning_rate=0.1),\n",
    "    GradientBoostingRegressor(max_depth=5, n_estimators=100, learning_rate=0.2),\n",
    "])\n",
    "scores[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot([_[0] for _ in scores], label=\"RandomForestRegressor\")\n",
    "ax.plot([_[1] for _ in scores], label=\"GBR(5, lr=0.05)\")\n",
    "ax.plot([_[2] for _ in scores], label=\"GBR(5, lr=0.1)\")\n",
    "ax.plot([_[3] for _ in scores], label=\"GBR(5, lr=0.2)\")\n",
    "ax.set_title(\"Comparaison pour différents learning_rate\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diminuer *learning_rate* est clairement une façon d'éviter le sur-apprentissage mais les graphes précédents ont montré qu'il fallait plus d'itérations lorsque le learning rate est petit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = experiment([\n",
    "    RandomForestRegressor(max_depth=5, n_estimators=100),\n",
    "    GradientBoostingRegressor(max_depth=1, n_estimators=100, learning_rate=0.05),\n",
    "    GradientBoostingRegressor(max_depth=1, n_estimators=100, learning_rate=0.1),\n",
    "    GradientBoostingRegressor(max_depth=1, n_estimators=100, learning_rate=0.2),\n",
    "])\n",
    "scores[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot([_[0] for _ in scores], label=\"RandomForestRegressor\")\n",
    "ax.plot([_[1] for _ in scores], label=\"GBR(1, lr=0.05)\")\n",
    "ax.plot([_[2] for _ in scores], label=\"GBR(1, lr=0.1)\")\n",
    "ax.plot([_[3] for _ in scores], label=\"GBR(1, lr=0.2)\")\n",
    "ax.set_title(\"Comparaison pour différents learning_rate et des fonctions en escalier\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus le modèle sous-jacent est simple, plus le *learning_rate* peut être élevé car les modèles simples ne font pas de sur-apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting avec d'autres librairies\n",
    "\n",
    "Une somme pondérée de régression linéaire reste une regréssion linéaire. Il est impossible de tester ce scénario avec *scikit-learn* puisque seuls les arbres de décisions sont implémentés. Mais il existe d'autres librairies qui implémente le gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = experiment([\n",
    "    RandomForestRegressor(max_depth=5, n_estimators=100),\n",
    "    XGBRegressor(max_depth=1, n_estimators=100, learning_rate=0.05, objective='reg:squarederror'),\n",
    "    XGBRegressor(max_depth=1, n_estimators=100, learning_rate=0.1, objective='reg:squarederror'),\n",
    "    XGBRegressor(max_depth=1, n_estimators=100, learning_rate=0.2, objective='reg:squarederror'),\n",
    "])\n",
    "scores[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot([_[0] for _ in scores], label=\"RandomForestRegressor(5)\")\n",
    "ax.plot([_[1] for _ in scores], label=\"XGB(1, lr=0.05)\")\n",
    "ax.plot([_[2] for _ in scores], label=\"XGB(1, lr=0.1)\")\n",
    "ax.plot([_[3] for _ in scores], label=\"XGB(1, lr=0.2)\")\n",
    "ax.set_title(\"Comparaison pour différents learning_rate\\net des fonctions en escalier \"\n",
    "             \"avec XGBoost\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont sensiblement les mêmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = experiment([\n",
    "    RandomForestRegressor(max_depth=5, n_estimators=100),\n",
    "    XGBRegressor(max_depth=5, n_estimators=100, learning_rate=0.05, objective='reg:squarederror'),\n",
    "    XGBRegressor(max_depth=5, n_estimators=100, learning_rate=0.1, objective='reg:squarederror'),\n",
    "    XGBRegressor(max_depth=5, n_estimators=100, learning_rate=0.2, objective='reg:squarederror'),\n",
    "])\n",
    "scores[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot([_[0] for _ in scores], label=\"RandomForestRegressor(5)\")\n",
    "ax.plot([_[1] for _ in scores], label=\"XGB(5, lr=0.05)\")\n",
    "ax.plot([_[2] for _ in scores], label=\"XGB(5, lr=0.1)\")\n",
    "ax.plot([_[3] for _ in scores], label=\"XGB(5, lr=0.2)\")\n",
    "ax.set_title(\"Comparaison pour différents learning_rate\\net des arbres de décisions \"\n",
    "             \"avec XGBoost\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = experiment([\n",
    "    RandomForestRegressor(max_depth=5, n_estimators=100),\n",
    "    LGBMRegressor(max_depth=1, n_estimators=100, learning_rate=0.05),\n",
    "    LGBMRegressor(max_depth=1, n_estimators=100, learning_rate=0.1),\n",
    "    LGBMRegressor(max_depth=1, n_estimators=100, learning_rate=0.2),\n",
    "])\n",
    "scores[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot([_[0] for _ in scores], label=\"RandomForestRegressor(5)\")\n",
    "ax.plot([_[1] for _ in scores], label=\"LGB(1, lr=0.05)\")\n",
    "ax.plot([_[2] for _ in scores], label=\"LGB(1, lr=0.1)\")\n",
    "ax.plot([_[3] for _ in scores], label=\"LGB(1, lr=0.2)\")\n",
    "ax.set_title(\"Comparaison pour différents learning_rate\\net des fonctions en escalier \"\n",
    "             \"avec LightGBM\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = experiment([\n",
    "    RandomForestRegressor(max_depth=5, n_estimators=100),\n",
    "    LGBMRegressor(max_depth=5, n_estimators=100, learning_rate=0.05),\n",
    "    LGBMRegressor(max_depth=5, n_estimators=100, learning_rate=0.1),\n",
    "    LGBMRegressor(max_depth=5, n_estimators=100, learning_rate=0.2),\n",
    "])\n",
    "scores[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot([_[0] for _ in scores], label=\"RandomForestRegressor(5)\")\n",
    "ax.plot([_[1] for _ in scores], label=\"LGB(5, lr=0.05)\")\n",
    "ax.plot([_[2] for _ in scores], label=\"LGB(5, lr=0.1)\")\n",
    "ax.plot([_[3] for _ in scores], label=\"LGB(5, lr=0.2)\")\n",
    "ax.set_title(\"Comparaison pour différents learning_rate\\net des arbres de décisions \"\n",
    "             \"avec LightGBM\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LightGBM](https://lightgbm.readthedocs.io/en/latest/) paraît moins sensible au *learning_rate* que [XGBoost](https://xgboost.readthedocs.io/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost\n",
    "\n",
    "[CatBoost](https://catboost.ai/) est une des plus récentes. Elle est sensée être plus efficace pour les catégories ce qui n'est pas le cas ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = experiment([\n",
    "    RandomForestRegressor(max_depth=5, n_estimators=100),\n",
    "    CatBoostRegressor(max_depth=1, n_estimators=100, learning_rate=0.05, verbose=False),\n",
    "    CatBoostRegressor(max_depth=1, n_estimators=100, learning_rate=0.1, verbose=False),\n",
    "    CatBoostRegressor(max_depth=1, n_estimators=100, learning_rate=0.2, verbose=False),\n",
    "])\n",
    "scores[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot([_[0] for _ in scores], label=\"RandomForestRegressor(5)\")\n",
    "ax.plot([_[1] for _ in scores], label=\"CAT(1, lr=0.05)\")\n",
    "ax.plot([_[2] for _ in scores], label=\"CAT(1, lr=0.1)\")\n",
    "ax.plot([_[3] for _ in scores], label=\"CAT(1, lr=0.2)\")\n",
    "ax.set_title(\"Comparaison pour différents learning_rate\\net des fonctions en escalier \"\n",
    "             \"avec CatBoost\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = experiment([\n",
    "    RandomForestRegressor(max_depth=5, n_estimators=100),\n",
    "    CatBoostRegressor(max_depth=5, n_estimators=100, learning_rate=0.05, verbose=False),\n",
    "    CatBoostRegressor(max_depth=5, n_estimators=100, learning_rate=0.1, verbose=False),\n",
    "    CatBoostRegressor(max_depth=5, n_estimators=100, learning_rate=0.2, verbose=False),\n",
    "])\n",
    "scores[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot([_[0] for _ in scores], label=\"RandomForestRegressor(5)\")\n",
    "ax.plot([_[1] for _ in scores], label=\"CAT(5, lr=0.05)\")\n",
    "ax.plot([_[2] for _ in scores], label=\"CAT(5, lr=0.1)\")\n",
    "ax.plot([_[3] for _ in scores], label=\"CAT(5, lr=0.2)\")\n",
    "ax.set_title(\"Comparaison pour différents learning_rate\\net des fonctions en escalier \"\n",
    "             \"avec CatBoost\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
